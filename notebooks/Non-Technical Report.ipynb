{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Recommendation for Stack Overflow users\n",
    "\n",
    "## Goal \n",
    "Stack Overflow users can be ranked based on the number of questions they answer and the cumulative scores for all their answers. The goal is to recommend jobs for a specific user by building a profile strength for that user. The profile strength for a user can be determined using characteristics of users such as their skills, answers they gave and user \"About Me\" information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "There are two main datasets used in this project. \n",
    "#### 1. The Users dataset\n",
    "The Users dataset is a combination of several datasets obtained from the Stack Overflow data dump. This dataset consists of Questions, Answers, associated scores and tags for each answer, user reputation and \"About Me\" information for each user. To understand how many answers were given by each user, I constructed a feature to count the number of answers given per user. All the questions and answers are based on computer science and programming.  \n",
    "\n",
    "The Stack Overflow data dump can be found at:\n",
    "* https://archive.org/details/stackexchange\n",
    "\n",
    "#### 2. The Job descriptions dataset\n",
    "\n",
    "The Job descriptions dataset was scrapped from online job boards. The goal was to retrieve all jobs related to Data science and Software Engineering. The following keywords were used in the search of all related job postings:\n",
    "1. Data scientist\n",
    "2. Data Engineer\n",
    "3. Data Analyst\n",
    "4. Data science\n",
    "5. Machine learning\n",
    "6. Business Analyst\n",
    "7. Software engineer\n",
    "8. Fullstack developer\n",
    "9. Frontend developer\n",
    "10. Backend developer\n",
    "\n",
    "The locations used for the search:\n",
    "1. San Francisco, CA\n",
    "2. Mountain View, CA\n",
    "3. Seattle, WA\n",
    "4. New York, NY\n",
    "5. Los Angeles, CA\n",
    "\n",
    "The goal of scrapping the job boards was to obtain a detailed job description which comprised of all the skills required for that job. I used Scrapy to write a web crawler to recursively parse through all the job posting links for a given search parameter and location. \n",
    "\n",
    "The intial problem I encountered with scrapping the job board was the inconsistency in the structure of the job descriptions. Some job posting links were hosted by the job board whereas some links redirected to an external website. To overcome this problem, I parsed through each of the links to obtain the entire html code for that particular job description. This enabled me to grab all the required information related to job description even if it was redirected to an external website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
